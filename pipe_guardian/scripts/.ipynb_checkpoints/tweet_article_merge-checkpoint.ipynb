{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make df linking articles to outlinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dictionary from colab of outlinks where headline is not the same as tweets, and the headline.\n",
    "# we can use this to link to article\n",
    "with open('../data/import_data/guardian_dic', 'rb') as f:\n",
    "    outlinks_headline=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_dic={}\n",
    "for k in list(outlinks_headline.keys()):\n",
    "    if isinstance(outlinks_headline[k], list):\n",
    "        raw =outlinks_headline[k][0]\n",
    "    else:\n",
    "        raw=outlinks_headline[k]\n",
    "    raw=raw.split(\"|\")[0].strip(\" \")\n",
    "    explode_dic[k] = raw\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlinks_headline_df=pd.DataFrame.from_dict(explode_dic, orient='index', columns=['headline_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlinks_headline_df.reset_index(inplace=True)\n",
    "outlinks_headline_df.rename(columns={'index': 'outlink'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlinks_headline_df=outlinks_headline_df[outlinks_headline_df['headline_link']!='Page Not Found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outlinks_headline_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_tweets = pd.read_csv('../data/guardian_tweets_df.csv', converters={\"Outlinks\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")}, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_tweets['Datetime']=guardian_tweets['Datetime'].apply(lambda x: x[:10])\n",
    "guardian_tweets.rename(columns={'Datetime': 'date'}, inplace=True)\n",
    "guardian_tweets.rename(columns={'Text': 'tweetText', 'Replies':'replies', 'Retweets': 'retweets', 'Likes':'likes'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_tweets=guardian_tweets.explode('Outlinks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tw in ['replies', 'retweets', 'likes']:\n",
    "    guardian_tweets[tw]=guardian_tweets[tw].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make df linking scraped articles to their tweets, if the headline isnt the same as the tweet. \n",
    "#First add the headline to the tweet df by linking through outlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_article_analysis=guardian_tweets.merge(outlinks_headline_df, left_on='Outlinks', right_on='outlink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_article_analysis.drop('Outlinks', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link the rest of the article using the headline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/guardian_articles_df', 'rb') as f:\n",
    "    guardian_scrape_articles=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_scrape_articles.rename(columns={'text':'artText'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_scrape_articles['date']=guardian_scrape_articles['date'].apply(lambda x:x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full article analysis is just the articles scraped from API. Pointless scraping articles from tweets outlinks\n",
    "# as the API returns all articles\n",
    "article_analysis=guardian_scrape_articles.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/guardian_articles_analysis', 'wb') as f:\n",
    "    pickle.dump(article_analysis, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tweets with articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_article_analysis=guardian_scrape_articles.merge(twitter_article_analysis, left_on='headline', right_on='headline_link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_article_analysis.drop(['headline_link', 'date_x', 'outlink'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_article_analysis.rename(columns={'date_y':'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have linked articles to tweets where tweet is differnt to the headline (maybe just a link after the tweet). \n",
    "# We need to add rows where they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/guardian_tw_art', 'rb') as f:\n",
    "    guardian_tw_art=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_tw_art.drop(['date_y', 'URL_list'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_tw_art.rename(columns={'text':'artText', 'Text':'tweetText'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "guardian_tw_art.rename(columns={'Replies':'replies', 'Retweets': 'retweets', 'Likes':'likes', 'date_x':'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tw in ['replies', 'retweets', 'likes']:\n",
    "    guardian_tw_art[tw]=guardian_tw_art[tw].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_tw_art['date']=guardian_tw_art['date'].apply(lambda x: x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_article_analysis=pd.concat([twitter_article_analysis, guardian_tw_art])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_article_analysis.drop_duplicates(subset='tweetText', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_article_analysis.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(x):\n",
    "    x=x.partition('http')[0].strip(' \"!:,.\"')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_article_analysis['tweetText']=twitter_article_analysis['tweetText'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/guardian_twitter_articles_analysis', 'wb') as f:\n",
    "    pickle.dump(twitter_article_analysis, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
